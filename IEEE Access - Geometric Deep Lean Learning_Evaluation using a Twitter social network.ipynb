{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IEEE Access. Geometric deep lean learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Set Package parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Calibri'\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelweight'] = 'normal'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['legend.labelspacing'] = 0.3\n",
    "sns.set_palette('muted')\n",
    "\n",
    "np.set_printoptions(edgeitems=30, \n",
    "                    linewidth=100000, \n",
    "                    formatter=dict(float=lambda x: \"%.3g\" % x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data pre--processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load collected re--tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "retweets_df = pd.read_pickle('all_retweets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Select the relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filtered_tweets = retweets_df[['retweeter_user_id', \n",
    "                               'retweeted_user_id', \n",
    "                               'hashtag', \n",
    "                               'group', \n",
    "                               'created_at', \n",
    "                               'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Clean and balance data--set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Delete the first and last 6 hours of tweets \n",
    "as this is the time it took to collect all tweets\"\"\"\n",
    "\n",
    "filtered_tweets = filtered_tweets[(filtered_tweets['created_at'] > min(filtered_tweets['created_at']) + datetime.timedelta(hours=6)) & (filtered_tweets['created_at'] < max(filtered_tweets['created_at']) - datetime.timedelta(hours=6))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Select hastag group #machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filtered_tweets = filtered_tweets[filtered_tweets['hashtag'] == '#machinelearning']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Hyper--parameter design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Temporal depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "min_time = min(filtered_tweets['created_at'])\n",
    "max_time = max(filtered_tweets['created_at']) \n",
    "time_length = max_time - min_time\n",
    "print('Start time',min_time,'End time',max_time,'duration',time_length)\n",
    "\n",
    "time_depth = 6\n",
    "\n",
    "t = time_depth/7\n",
    "\n",
    "time_end_training = min_time + time_length * t\n",
    "print(time_end_training)\n",
    "print('Duration training',time_end_training-min_time)\n",
    "\n",
    "time_start_test = time_end_training\n",
    "time_end_test = min_time + time_length\n",
    "print(time_end_test)\n",
    "print('Duration test',time_end_test-time_start_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Spatial depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Three layers with weights \n",
    "that are initialized with weights between 0 and 0.1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights_1 = np.random.rand(len(training_ids),len(training_ids))*0.1\n",
    "weights_2 = np.random.rand(len(training_ids),len(training_ids))*0.1\n",
    "weights_3 = np.random.rand(len(training_ids),len(training_ids))*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Filter Data--set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_tweets = filtered_tweets[filtered_tweets['created_at'] <= time_end_training].reset_index()\n",
    "\n",
    "test_tweets = filtered_tweets[(filtered_tweets['created_at'] > time_end_training) & (filtered_tweets['created_at'] <= time_end_test)].reset_index()\n",
    "\n",
    "training_ids = set(training_tweets['retweeted_user_id'].tolist()+training_tweets['retweeter_user_id'].tolist())\n",
    "print(len(training_ids),'unique IDs for the training retweets')\n",
    "\n",
    "test_ids = set(test_tweets['retweeted_user_id'].tolist()+test_tweets['retweeter_user_id'].tolist())\n",
    "print(len(test_ids),'unique IDs for all of the test retweets')\n",
    "\n",
    "used_ids = list(training_ids)\n",
    "\n",
    "used_ids.sort(key=int)\n",
    "\n",
    "filtered_test_tweets = test_tweets[test_tweets['retweeted_user_id'].isin(training_ids) & test_tweets['retweeter_user_id'].isin(training_ids)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Re--tweet content cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def string_alphabet_to_np_float(string):\n",
    "    '''String that contains only alphabetics to float between >0 and 1'''\n",
    "    cleaned_string = clean_str(string)\n",
    "    return (np.frombuffer(cleaned_string.encode(), np.int8) - ord('a') + 1)/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    '''Only letters from string'''\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    string = re.sub(r'[\\W_]+', '', string)\n",
    "    string = re.sub(r'[0-9]', '', string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filtered_validation_tweets = validation_tweets[validation_tweets['retweeted_user_id'].isin(training_ids) & validation_tweets['retweeter_user_id'].isin(training_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"To make the access to the weights easier, \n",
    "the IDs are changed to an index starting at 0\"\"\"\n",
    "\n",
    "training_tweets['retweeter_user_index'] = training_tweets['retweeter_user_id'].apply(used_ids.index)\n",
    "\n",
    "training_tweets['retweeted_user_index'] = training_tweets['retweeted_user_id'].apply(used_ids.index)\n",
    "\n",
    "filtered_test_tweets['retweeter_user_index'] = filtered_test_tweets['retweeter_user_id'].apply(used_ids.index)\n",
    "\n",
    "filtered_test_tweets['retweeted_user_index'] = filtered_test_tweets['retweeted_user_id'].apply(used_ids.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Re--tweet content transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Naive Bayes sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "blob_object = TextBlob(filtered_tweets['text'][502450], \n",
    "                       analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "print('How positiv is the sentiment of the previous tweet?',\n",
    "      blob_object.sentiment.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_positive_sentiment_from_text(text, length):\n",
    "    '''Get positive sentiment from text. Minimum is set to 0.001. Maximum is 1'''\n",
    "    blob_object = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
    "    return max(blob_object.sentiment.p_pos,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_positive_sentiment_and_categories_from_text(text, length):\n",
    "    '''Get positive sentiment from text. Minimum is set to 0.001. Maximum is 1'''\n",
    "    blob_object = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
    "            \n",
    "    floats_of = string_alphabet_to_np_float(text)\n",
    "    if floats_of.shape[0] < 240:\n",
    "        floats_of = np.concatenate((floats_of, np.array([0] * (240 - len(floats_of)))), axis=0)\n",
    "\n",
    "    categories = model.predict(np.array( [floats_of,] ))\n",
    "    ad = [max(blob_object.sentiment.p_pos,0.001)] + list(categories[0])\n",
    "    return np.array(ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Most used words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "most_used_words = Counter()\n",
    "for text in training_tweets['text']:\n",
    "    for t in text.lower().split():\n",
    "        most_used_words[t] += 1\n",
    "        \n",
    "top_used = most_used_words.most_common(10)\n",
    "top_used_names = [t[0] for t in top_used]\n",
    "top_used_values = [t[1] for t in top_used]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reduced Lapalacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_adjacency_dict(df, direction):\n",
    "    '''Create adjacency dictionary through the connections created by retweets'''\n",
    "    connection_one = defaultdict(set)\n",
    "    for i in range(len(df)):\n",
    "        if direction == 'undirected' or direction == 'retweeted':\n",
    "            connection_one[df.iloc[i]['retweeter_user_index']].add(df.iloc[i]['retweeted_user_index'])\n",
    "        if direction == 'undirected' or direction == 'retweeter':\n",
    "            connection_one[df.iloc[i]['retweeted_user_index']].add(df.iloc[i]['retweeter_user_index'])\n",
    "    return connection_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_nth_neighbour_for_adjacency_dict(direct_neighbours, neighbour_depth):\n",
    "    '''Calculate the nth neighbour of a network'''\n",
    "    new_one = deepcopy(direct_neighbours)\n",
    "    new_dict = deepcopy(direct_neighbours)\n",
    "    \n",
    "    for n in range(neighbour_depth - 1):\n",
    "        for node, connected in new_one.items():\n",
    "            for con in connected:\n",
    "                if con in direct_neighbours:\n",
    "                    new_dict[node].update(direct_neighbours[con])\n",
    "            new_dict[node].discard(node)\n",
    "        new_one = deepcopy(new_dict)\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def adjacency_dict_to_matrix(base_nodes, adj_dict, directed):\n",
    "    '''Create adjacency matrix from adjacency dictionary'''\n",
    "    if directed:\n",
    "        new_graph = nx.DiGraph()\n",
    "    else:\n",
    "        new_graph = nx.Graph()\n",
    "    for node in base_nodes:\n",
    "        new_graph.add_node(node)\n",
    "    for node, connected in adj_dict.items():\n",
    "        for con in connected:\n",
    "            new_graph.add_edge(node, con, weight=1)\n",
    "    adjacency_matrix = nx.adjacency_matrix(new_graph)\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def undirected_adjacency_dict_to_normalized_laplacian(base_nodes, adj_dict):\n",
    "    '''Create normalized laplacian from adjacency dictionary'''\n",
    "    new_graph = nx.Graph()\n",
    "    \n",
    "    for node in base_nodes:\n",
    "        new_graph.add_node(node)\n",
    "\n",
    "    for node, connected in adj_dict.items():\n",
    "        for con in connected:\n",
    "            new_graph.add_edge(node, con, weight=1)\n",
    "    normalizedlaplacian_matrix = nx.normalized_laplacian_matrix(new_graph)\n",
    "    return normalizedlaplacian_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def undirected_adjacency_dict_to_subclusters(adj_dict, number_of_clusters, only_if_both_here):\n",
    "    '''Create normalized laplacian from adjacency dictionary'''\n",
    "    new_graph = nx.Graph()\n",
    "    \n",
    "    node_to_group = dict()\n",
    "    \n",
    "    for node, connected in adj_dict.items():\n",
    "        for con in connected:\n",
    "            if len(only_if_both_here) > 0:\n",
    "                if node in only_if_both_here and con in only_if_both_here:\n",
    "                    new_graph.add_edge(node, con, weight=1)\n",
    "            else:\n",
    "                new_graph.add_edge(node, con, weight=1)\n",
    "    \n",
    "    sub_graphs = [new_graph.subgraph(c) for c in nx.connected_components(new_graph)]\n",
    "    \n",
    "    for sub_graph in sub_graphs[1:]:\n",
    "        for node in sub_graph:\n",
    "            node_to_group[node] = number_of_clusters\n",
    "\n",
    "    adj_matrix = nx.to_numpy_matrix(sub_graphs[0]) #Converts graph to an adj matrix with adj_matrix[i][j] represents weight between node i,j.\n",
    "    node_list = list(sub_graphs[0].nodes()) #returns a list of nodes with index mapping with the a \n",
    "\n",
    "    clusters = SpectralClustering(affinity = 'precomputed', assign_labels=\"kmeans\",random_state=0,n_clusters=number_of_clusters).fit_predict(adj_matrix)\n",
    "    \n",
    "    for i, node in enumerate(node_list):\n",
    "        node_to_group[node] = clusters[i]\n",
    "    #print('Clusters',len(clusters))\n",
    "    #for i in range(n):\n",
    "    #    print(\"Subgraph:\", i, \"consists of \",len(sub_graphs[i].nodes()))\n",
    "    return node_to_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tweets_to_connection_matrix(tweets, ids_length):\n",
    "    '''Create connection matrix from all tweets'''\n",
    "    connections = np.zeros((ids_length, ids_length), dtype=np.bool)\n",
    "    \n",
    "    adj_dict = create_adjacency_dict(tweets, 'undirected')\n",
    "    \n",
    "    for a in adj_dict:\n",
    "        for b in adj_dict[a]:\n",
    "            connections[a,b] = 1\n",
    "            connections[b,a] = 1\n",
    "    \n",
    "    return connections.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Source Code\n",
    "@article{dallamico_unified_2020,\n",
    "\ttitle = {A unified framework for spectral clustering in sparse graphs}\n",
    "    url = {http://arxiv.org/abs/2003.09198}\"\"\"\n",
    "\n",
    "def lap(A,classes, assortativity,number_eig):\n",
    "\n",
    "    ''' This function performs spectral clustering on the best eigenvector of the random walk laplacian matrix D^{-1}A. \n",
    "\n",
    "    Use: y_kmeans,eigenvalues, X, ov = lap(A,classes, assortativity,number_eig)\n",
    "    Inputs: A a (symmetric) n x n adjacency matrix of a graph, classes a vector of size n with the underlying ground truth class assignment and assortativity is set to 1 if one seeks assortative blocks or to -1 if one seeks disassortative blocks, number_eig is the number of eigenvalues among which look for the best \n",
    "    Outputs: y_kmeans the vector of size n of detected classes, eigenvalues the values of the two smallest (largest) eigenvalues of A, X the eigenvector of size n used for the classification and ov the overlap between the detected classes y_kmeans and the ground truth classes.\n",
    "\n",
    "    '''\n",
    "\n",
    "    n = len(A)\n",
    "    A = A.astype(float)\n",
    "    d = np.sum(A,axis = 0)\n",
    "    D_1 = np.diag(d**(-1))\n",
    "    L = np.dot(D_1,A)\n",
    "\n",
    "    if assortativity > 0:\n",
    "        eigenvalues,eigenvectors = scipy.sparse.linalg.eigs(L, number_eig, which='LR')\n",
    "    else:\n",
    "        eigenvalues,eigenvectors = scipy.sparse.linalg.eigs(L, number_eig, which='SR')\n",
    "\n",
    "    eigenvalues = eigenvalues.real # the matrix is  symmetric so the eigenvalues are real\n",
    "    eigenvectors = eigenvectors.real # the matrix is  symmetric so the eigenvectors are real\n",
    "    idx = eigenvalues.argsort()[::-1] # order the eigenvectors\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:,idx]\n",
    "\n",
    "    current_ov = 0\n",
    "    y = np.zeros(n)\n",
    "    vector = np.zeros((len(eigenvectors[:,0]),2))\n",
    "\n",
    "    # This cycle picks the best out of number_eig computed eigenvectors\n",
    "\n",
    "    for i in range(1,number_eig):\n",
    "\n",
    "        X = np.ones(len(eigenvectors[:,0]))\n",
    "        X = np.column_stack((X,eigenvectors[:,i]))\n",
    "        kmeans = KMeans(n_clusters = 2)\n",
    "        kmeans.fit(X)\n",
    "        y_kmeans = kmeans.predict(X)\n",
    "        precision1 = overlap(y_kmeans,classes) # choose between the class assignment 0 -> A, 1 -> B and 0 -> B, 1 -> A, A and B being the two classes. This keeps the overlap positive\n",
    "        precision2 = overlap(1-y_kmeans,classes)\n",
    "        ov = max(precision1, precision2)\n",
    "        if ov > current_ov:\n",
    "            current_ov = ov\n",
    "            y = y_kmeans\n",
    "            vector = np.zeros(len(eigenvectors[:,0]))\n",
    "            vector = np.column_stack((vector,eigenvectors[:,i]))\n",
    "            if ov == precision2:\n",
    "                y = 1-y_kmeans\n",
    "\n",
    "    return y,eigenvalues, vector, current_ov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Re-tweet search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_tweet(df, user_index_a, user_index_b):\n",
    "    '''Find tweet between both users'''\n",
    "    return df[((df['retweeter_user_index'] == user_index_a) & (df['retweeted_user_index'] == user_index_b)) | ((df['retweeted_user_index'] == user_index_a) & (df['retweeter_user_index'] == user_index_b))]['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_tweet_only_from(df, user_index_b):\n",
    "    '''Find tweet from that user'''\n",
    "    return df[(df['retweeted_user_index'] == user_index_b) | (df['retweeter_user_index'] == user_index_b)]['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_all_tweet_connected_to(df, user_indexes_b):\n",
    "    '''Find tweet from that user'''\n",
    "    return df[(df['retweeted_user_index'].isin(user_indexes_b)) | (df['retweeter_user_index'].isin(user_indexes_b))]['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Re-tweet content length normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def array_of_strings_to_float_with_length(strings, needed_length):\n",
    "    '''Take all strings. Transform these into floats and adapt them, to fit a specified length'''\n",
    "    new_strings = np.array([])\n",
    "    for i in range(strings.shape[0]):\n",
    "        conv_str = string_alphabet_to_np_float(strings[i])\n",
    "        if new_strings.shape[0] == 0:\n",
    "            new_strings = np.transpose(get_needed_length_of_array(conv_str, needed_length))\n",
    "            new_strings = new_strings.reshape(new_strings.shape[0], 1)\n",
    "        else:\n",
    "            new_strings = np.concatenate((new_strings, np.transpose(get_needed_length_of_array(conv_str, needed_length)).reshape(new_strings.shape[0], 1)), axis=1)\n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Transform re--tweet conent to constant for research purposes\"\"\"\n",
    "\n",
    "def return_constant_string(length):\n",
    "    return np.array([0.5]*length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def array_of_strings_to_float_with_length_random(strings, needed_length):\n",
    "    '''Take all strings. Transform these into floats and adapt them, to fit a specified length'''\n",
    "    new_strings = np.array([])\n",
    "    for i in range(strings.shape[0]):\n",
    "        conv_str = return_random_string(240)\n",
    "        if new_strings.shape[0] == 0:\n",
    "            new_strings = np.transpose(get_needed_length_of_array(conv_str, needed_length))\n",
    "            new_strings = new_strings.reshape(new_strings.shape[0], 1)\n",
    "        else:\n",
    "            new_strings = np.concatenate((new_strings, np.transpose(get_needed_length_of_array(conv_str, needed_length)).reshape(new_strings.shape[0], 1)), axis=1)\n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def array_of_strings_to_float_with_length_sentiment_and_categories(strings, needed_length):\n",
    "    '''Take all strings. Transform these into sentiment and categories and adapt them, to fit a specified length'''\n",
    "    new_strings = np.array([])\n",
    "    for i in range(strings.shape[0]):\n",
    "        conv_str = get_positive_sentiment_and_categories_from_text(strings[i], 240)\n",
    "        if new_strings.shape[0] == 0:\n",
    "            new_strings = np.transpose(get_needed_length_of_array(conv_str, needed_length))\n",
    "            new_strings = new_strings.reshape(new_strings.shape[0], 1)\n",
    "        else:\n",
    "            new_strings = np.concatenate((new_strings, np.transpose(get_needed_length_of_array(conv_str, needed_length)).reshape(new_strings.shape[0], 1)), axis=1)\n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_needed_length_of_array(arr, needed_length):\n",
    "    '''Adapt array to specific length. If it is too short, duplicate it and if it is too long, cut it'''\n",
    "    if arr.shape[0] >= needed_length:\n",
    "        return arr[:needed_length]\n",
    "    else:\n",
    "        repeats_needed = (needed_length // arr.shape[0]) + 1\n",
    "        tiled = np.tile(arr, repeats_needed)\n",
    "        return tiled[:needed_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Geometric deep lean learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Activation function for convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu(array):\n",
    "    return np.maximum(0, array)\n",
    "\n",
    "def relu_backwards(array, Z):\n",
    "    dZ = np.array(array, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Node cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster_to_nodes = [[] for i in range(11)]\n",
    "for key in node_to_cluster.keys():\n",
    "    cluster_to_nodes[node_to_cluster[key]].append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Geometric deep lean learning convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The neural network is based on multiple layers. \n",
    "Layer one considers all neighbours with distance 1 from node a. \n",
    "Layer two all neighbours with distance 2 from node a and layer \n",
    "three all neighbours with distance 3. \n",
    "The training is done by minimizing the loss between the results \n",
    "of the third and second layer and then of the second with \n",
    "the first layer. These steps are performed multiple time for all results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "time_length = 5\n",
    "learning_rate = 0.08\n",
    "global_iterations = 5\n",
    "local_iterations = 50\n",
    "validation_losses = []\n",
    "weights_changes = []\n",
    "sum_of_all_losses = 0\n",
    "\n",
    "weights_changes.append(weights_1.copy())\n",
    "\n",
    "maximum_minutes_validation = int((max(filtered_validation_tweets['created_at']) - min(filtered_validation_tweets['created_at'])).total_seconds()/60)\n",
    "for cluster in cluster_to_nodes:\n",
    "    for minutes_started in range(0, maximum_minutes_validation, time_length):\n",
    "        time_slice = filtered_validation_tweets[(filtered_validation_tweets['created_at'] < min(filtered_validation_tweets['created_at']) + datetime.timedelta(minutes=minutes_started+time_length)) & (filtered_validation_tweets['created_at'] >= min(filtered_validation_tweets['created_at']) + datetime.timedelta(minutes=minutes_started))]\n",
    "\n",
    "        time_slice = time_slice[time_slice['retweeted_user_index'].isin(cluster) & time_slice['retweeter_user_index'].isin(cluster)]\n",
    "        \n",
    "        if len(time_slice) == 0:\n",
    "            continue\n",
    "            \n",
    "        all_ids = list(set(time_slice['retweeted_user_index'].tolist() + time_slice['retweeter_user_index'].tolist()))\n",
    "\n",
    "        adj_dict_n1 = get_nth_neighbour_for_adjacency_dict(create_adjacency_dict(time_slice, 'undirected'), 1)\n",
    "        adj_matrix_n1 = adjacency_dict_to_matrix(all_ids, adj_dict_n1, True)\n",
    "\n",
    "        adj_dict_n2 = get_nth_neighbour_for_adjacency_dict(create_adjacency_dict(time_slice, 'undirected'), 2)\n",
    "        adj_matrix_n2 = adjacency_dict_to_matrix(all_ids, adj_dict_n2, True)\n",
    "\n",
    "        laplacian_n1 = undirected_adjacency_dict_to_normalized_laplacian(all_ids, adj_dict_n1)\n",
    "        eigen_values_n1, eigen_vectors_n1 = np.linalg.eigh(laplacian_n1.toarray())\n",
    "\n",
    "        laplacian_n2 = undirected_adjacency_dict_to_normalized_laplacian(all_ids, adj_dict_n2)\n",
    "        eigen_values_n2, eigen_vectors_n2 = np.linalg.eigh(laplacian_n2.toarray())\n",
    "\n",
    "        for a in all_ids:\n",
    "            if len(adj_dict_n1[a]) == 0:\n",
    "                continue\n",
    "\n",
    "            za_parts = []\n",
    "            needed_tweets_n1 = find_all_tweet_connected_to(time_slice, adj_dict_n1[a])\n",
    "            conv_strings_n1 = array_of_strings_to_float_with_length_sentiment_and_categories(needed_tweets_n1, laplacian_n1.shape[0])\n",
    "\n",
    "            na_za_n1 = np.array([weights_1[a, b]*np.matmul(array_n(laplacian_n1.toarray(),i), conv_strings_n1) for i, b in enumerate(adj_dict_n1[a])])\n",
    "            na_za_n1 = sum(na_za_n1)\n",
    "            za_n1 = relu(na_za_n1)\n",
    "\n",
    "            na_za_n2 = [weights_2[a, b]*np.matmul(array_n(eigen_vectors_n2,i), za_n1) for i, b in enumerate(adj_dict_n2[a])]\n",
    "            na_za_n2 = sum(na_za_n2)\n",
    "            za_n2 = relu(na_za_n2)\n",
    "\n",
    "            try:\n",
    "                delta_loss = (za_n1 - za_n2) * relu_backwards(za_n2, na_za_n1)\n",
    "                sum_of_all_losses += np.sum(np.abs(delta_loss))/(delta_loss.size)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print('loss',sum_of_all_losses)\n",
    "validation_losses.append(sum_of_all_losses)\n",
    "\n",
    "for global_iteration in range(global_iterations):\n",
    "    sum_of_all_losses = 0\n",
    "\n",
    "    for cluster in cluster_to_nodes:\n",
    "        maximum_minutes_training = int((max(training_tweets['created_at']) - min(training_tweets['created_at'])).total_seconds()/60)\n",
    "        for minutes_started in range(0, maximum_minutes_training, time_length):\n",
    "            time_slice = training_tweets[(training_tweets['created_at'] < min(training_tweets['created_at']) + datetime.timedelta(minutes=minutes_started+time_length)) & (training_tweets['created_at'] >= min(training_tweets['created_at']) + datetime.timedelta(minutes=minutes_started))]\n",
    "\n",
    "            time_slice = time_slice[time_slice['retweeted_user_index'].isin(cluster) & time_slice['retweeter_user_index'].isin(cluster)]\n",
    "        \n",
    "            if len(time_slice) == 0:\n",
    "                continue\n",
    "\n",
    "            all_ids = list(set(time_slice['retweeted_user_index'].tolist() + time_slice['retweeter_user_index'].tolist()))\n",
    "\n",
    "            adj_dict_n1 = get_nth_neighbour_for_adjacency_dict(create_adjacency_dict(time_slice, 'undirected'), 1)\n",
    "            adj_matrix_n1 = adjacency_dict_to_matrix(all_ids, adj_dict_n1, True)\n",
    "\n",
    "            adj_dict_n2 = get_nth_neighbour_for_adjacency_dict(create_adjacency_dict(time_slice, 'undirected'), 2)\n",
    "            adj_matrix_n2 = adjacency_dict_to_matrix(all_ids, adj_dict_n2, True)\n",
    "\n",
    "            adj_dict_n3 = get_nth_neighbour_for_adjacency_dict(create_adjacency_dict(time_slice, 'undirected'), 3)\n",
    "            adj_matrix_n3 = adjacency_dict_to_matrix(all_ids, adj_dict_n3, True)\n",
    "\n",
    "            laplacian_n1 = undirected_adjacency_dict_to_normalized_laplacian(all_ids, adj_dict_n1)\n",
    "            eigen_values_n1, eigen_vectors_n1 = np.linalg.eigh(laplacian_n1.toarray())\n",
    "\n",
    "            laplacian_n2 = undirected_adjacency_dict_to_normalized_laplacian(all_ids, adj_dict_n2)\n",
    "            eigen_values_n2, eigen_vectors_n2 = np.linalg.eigh(laplacian_n2.toarray())\n",
    "\n",
    "            laplacian_n3 = undirected_adjacency_dict_to_normalized_laplacian(all_ids, adj_dict_n3)\n",
    "            eigen_values_n3, eigen_vectors_n3 = np.linalg.eigh(laplacian_n3.toarray())\n",
    "\n",
    "            for a in all_ids:\n",
    "                if len(adj_dict_n1[a]) == 0:\n",
    "                    continue\n",
    "\n",
    "                za_parts = []\n",
    "                needed_tweets_n1 = find_all_tweet_connected_to(time_slice, adj_dict_n1[a])\n",
    "                conv_strings_n1 = array_of_strings_to_float_with_length_sentiment_and_categories(needed_tweets_n1, laplacian_n1.shape[0])\n",
    "\n",
    "                na_za_n1 = np.array([weights_1[a, b]*np.matmul(array_n(laplacian_n1.toarray(),i), conv_strings_n1) for i, b in enumerate(adj_dict_n1[a])])\n",
    "                na_za_n1 = sum(na_za_n1)\n",
    "                za_n1 = relu(na_za_n1)\n",
    "\n",
    "                na_za_n2 = [weights_2[a, b]*np.matmul(array_n(eigen_vectors_n2,i), za_n1) for i, b in enumerate(adj_dict_n2[a])]\n",
    "                na_za_n2 = sum(na_za_n2)\n",
    "                za_n2 = relu(na_za_n2)\n",
    "\n",
    "                na_za_n3 = [weights_3[a, b]*np.matmul(array_n(eigen_vectors_n3,i), za_n2) for i, b in enumerate(adj_dict_n3[a])]\n",
    "                na_za_n3 = sum(na_za_n3)\n",
    "                za_n3 = relu(na_za_n3)\n",
    "\n",
    "                for iterations in range(local_iterations):\n",
    "                    delta_loss = (za_n3 - za_n2) * relu_backwards(za_n3, na_za_n2)\n",
    "\n",
    "                    if np.sum(np.abs(delta_loss))/(delta_loss.size) < 0.001:\n",
    "                        break\n",
    "\n",
    "                    for i, b in enumerate(adj_dict_n2[a]):\n",
    "                        change = learning_rate * np.matmul(delta_loss[all_ids.index(b),:], za_n3[all_ids.index(b),:])\n",
    "                        weights_2[a, b] = max(0, min(1, weights_2[a, b] + change))\n",
    "\n",
    "                    na_za_n2 = [weights_2[a, b]*np.matmul(array_n(eigen_vectors_n2,i), za_n1) for i, b in enumerate(adj_dict_n2[a])]\n",
    "                    na_za_n2 = sum(na_za_n2)\n",
    "                    za_n2 = relu(na_za_n2)\n",
    "\n",
    "                for iterations in range(local_iterations):\n",
    "                    try:\n",
    "                        delta_loss = (za_n2 - za_n1) * relu_backwards(za_n2, na_za_n1)\n",
    "                    except:\n",
    "                        break\n",
    "\n",
    "                    if np.sum(np.abs(delta_loss))/(delta_loss.size) < 0.001:\n",
    "                        break\n",
    "\n",
    "                    for i, b in enumerate(adj_dict_n2[a]):\n",
    "                        change = learning_rate * np.matmul(delta_loss[all_ids.index(b),:], za_n2[all_ids.index(b),:])\n",
    "                        weights_1[a, b] = max(0, min(1, weights_1[a, b] + change))\n",
    "\n",
    "                    na_za_n1 = [weights_1[a, b]*np.matmul(array_n(laplacian_n1.toarray(),i), conv_strings_n1) for i, b in enumerate(adj_dict_n1[a])]\n",
    "                    na_za_n1 = sum(na_za_n1)\n",
    "                    za_n1 = relu(na_za_n1)\n",
    "\n",
    "        maximum_minutes_validation = int((max(filtered_validation_tweets['created_at']) - min(filtered_validation_tweets['created_at'])).total_seconds()/60)\n",
    "        for minutes_started in range(0, maximum_minutes_validation, time_length):\n",
    "            time_slice = filtered_validation_tweets[(filtered_validation_tweets['created_at'] < min(filtered_validation_tweets['created_at']) + datetime.timedelta(minutes=minutes_started+time_length)) & (filtered_validation_tweets['created_at'] >= min(filtered_validation_tweets['created_at']) + datetime.timedelta(minutes=minutes_started))]\n",
    "            time_slice = time_slice[time_slice['retweeted_user_index'].isin(cluster) & time_slice['retweeter_user_index'].isin(cluster)]\n",
    "\n",
    "            if len(time_slice) == 0:\n",
    "                continue\n",
    "\n",
    "            all_ids = list(set(time_slice['retweeted_user_index'].tolist() + time_slice['retweeter_user_index'].tolist()))\n",
    "\n",
    "            adj_dict_n1 = get_nth_neighbour_for_adjacency_dict(create_adjacency_dict(time_slice, 'undirected'), 1)\n",
    "            adj_matrix_n1 = adjacency_dict_to_matrix(all_ids, adj_dict_n1, True)\n",
    "\n",
    "            adj_dict_n2 = get_nth_neighbour_for_adjacency_dict(create_adjacency_dict(time_slice, 'undirected'), 2)\n",
    "            adj_matrix_n2 = adjacency_dict_to_matrix(all_ids, adj_dict_n2, True)\n",
    "\n",
    "            laplacian_n1 = undirected_adjacency_dict_to_normalized_laplacian(all_ids, adj_dict_n1)\n",
    "            eigen_values_n1, eigen_vectors_n1 = np.linalg.eigh(laplacian_n1.toarray())\n",
    "\n",
    "            laplacian_n2 = undirected_adjacency_dict_to_normalized_laplacian(all_ids, adj_dict_n2)\n",
    "            eigen_values_n2, eigen_vectors_n2 = np.linalg.eigh(laplacian_n2.toarray())\n",
    "\n",
    "            for a in all_ids:\n",
    "                if len(adj_dict_n1[a]) == 0:\n",
    "                    continue\n",
    "\n",
    "                za_parts = []\n",
    "                needed_tweets_n1 = find_all_tweet_connected_to(time_slice, adj_dict_n1[a])\n",
    "                conv_strings_n1 = array_of_strings_to_float_with_length_sentiment_and_categories(needed_tweets_n1, laplacian_n1.shape[0])\n",
    "\n",
    "                na_za_n1 = np.array([weights_1[a, b]*np.matmul(array_n(laplacian_n1.toarray(),i), conv_strings_n1) for i, b in enumerate(adj_dict_n1[a])])\n",
    "                na_za_n1 = sum(na_za_n1)\n",
    "                za_n1 = relu(na_za_n1)\n",
    "\n",
    "                na_za_n2 = [weights_2[a, b]*np.matmul(array_n(eigen_vectors_n2,i), za_n1) for i, b in enumerate(adj_dict_n2[a])]\n",
    "                na_za_n2 = sum(na_za_n2)\n",
    "                za_n2 = relu(na_za_n2)\n",
    "\n",
    "                try:\n",
    "                    delta_loss = (za_n2 - za_n1) * relu_backwards(za_n2, na_za_n1)\n",
    "                    sum_of_all_losses += np.sum(np.abs(delta_loss))/(delta_loss.size)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    print('loss',sum_of_all_losses)\n",
    "    validation_losses.append(sum_of_all_losses)\n",
    "    weights_changes.append(weights_1.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
